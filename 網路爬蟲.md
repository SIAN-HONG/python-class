# 網路爬蟲
Python提供webbrowser模組，可以呼叫這個模組的open()方法，就可以開啟指定網頁。
   ```Python
   import webbrowser
   webbrowser.open('http://www.mcut.edu.tw')
   ```
### 下載網頁資訊使用request模組
#### 下載網頁使用requests.get()方法
requests.get()方法內需放置欲下載網頁資訊的網址當參數，回傳網頁的response物件相關資訊。
   ```Python
   import requests
   url = 'http://www.mcut.edu.tw'
   response = requests.get(url)
   print(type(response))
   ```
#### 認識Response物件
Response物件內有下列幾個重要屬性:
   status_code : 如果值是requests.code.ok，表示獲得的網頁內容成功  
   url : 列出請求的網址  
   text : 網頁內容  
   content : 可獲得bytes類型的資料  
   headers : 表頭內容  
   cookies : cookies 內容  
   ```Python
   import requests

   url = 'http://www.mcut.edu.tw'
   response = requests.get(url)
   if response.status_code == requests.codes.ok:
       print("取得網頁內容成功")
       print("網址訊息   : ", response.url)
       print("表頭訊息   : ", response.headers)
       print("cookie訊息 : ", response.cookies)
   else:
       print("取得網頁內容失敗")
   ```
   ```Python
   import requests

   url = 'http://www.mcut.edu.tw'
   response = requests.get(url)
   if response.status_code == requests.codes.ok:
       print("取得網頁內容成功")
       print("網頁內容大小 = ", len(response.text))
   else:
       print("取得網頁內容失敗")

   ```
   ```Python
   import requests

   url = 'http://www.mcut.edu.tw'
   response = requests.get(url)
   if response.status_code == requests.codes.ok:
       print("取得網頁內容成功")
       print(response.text)            # 列印網頁內容
   else:
       print("取得網頁內容失敗")
   ```
#### 搜尋網頁特定內容
   ```Python
   import requests
   import re

   url = 'http://www.mcut.edu.tw'
   response = requests.get(url)
   if response.status_code == requests.codes.ok:
       pattern = input("請輸入欲搜尋的字串 : ")    # pattern存放欲搜尋的字串
   # 使用方法1
       if pattern in response.text:                # 方法1
           print("搜尋 %s 成功" % pattern)
       else:
           print("搜尋 %s 失敗" % pattern)
       # 使用方法2, 如果找到放在串列name內
       name = re.findall(pattern, response.text)   # 方法2
       if name != None:
           print("%s 出現 %d 次" % (pattern, len(name)))
       else:
           print("%s 出現 0 次" % pattern)
   else:
       print("網頁下載失敗")
   ```
#### 下載網頁的失敗處理
   ```Python
   import requests

   url = 'http://www.mcut.edu.tw/file_not_existed'  # 不存在的內容
   response = requests.get(url)
   try:
       response.raise_for_status()                 # 異常處理
       print("下載成功")
   except Exception as err:                        # err是系統自訂的錯誤訊息
       print("網頁下載失敗: %s" % err)
   print("程式結束")
   ```
   ```Python
   import requests

   url = 'http://www.gzaxxc.com/file_not_existed'  # 錯誤的網址
   response = requests.get(url)
   try:
       response.raise_for_status()                 # 異常處理
       print("下載成功")
   except Exception as err:                        # err是系統自訂的錯誤訊息
       print("網頁下載失敗: %s" % err)
   print("程式結束")
   ```

#### 網頁伺服器阻擋造成讀取錯誤
   ```Python
   import requests

   url = 'http://aaa.24ht.com.tw/'
   response = requests.get(url)
   response.raise_for_status()
   ```
   ```Python
   import requests

   url = 'https://www.kingstone.com.tw/basic/2013120599512?zone=book&lid=search&actid=WISE'
   response = requests.get(url)
   response.raise_for_status()   
   ```
   
### 認識robots.txt
robots.txt是一個儲存在網站根目錄下的文字格式檔案，因為在網路上的URL習慣對於大小寫是敏感的，所以統一用小寫。在robots.txt中會告訴你哪些內容不可以取得，或是哪些內容可以取得，也可以知道允許哪些特定網路爬蟲是可以爬的。  
**注意 : robots.txt不是一個規範，而是被長久使用約定俗成的習慣。**  

**實例1：User-Agent符號是'\*'表示針對所有爬蟲，允許所有的網路爬蟲。**  
User-Agent: \*  
Disallow:  

**實例2：允許特定網路爬蟲Applebot。**  
User-Agent: Applebot  
Disallow:  

**實例3：攔截所有網路爬蟲。**  
User-Agent: \*  
Disallow: /  

**實例4：攔截所有網路爬蟲拜訪特定目錄。**  
User-Agent: \*  
Disallow: /cgi-bin/  
Disallow: /private/  

**實例5：攔截所有網路爬蟲拜訪特定檔案。**  
User-Agent: \*  
Disallow: /\*.php$  
Disallow: /\*.css$  
  
